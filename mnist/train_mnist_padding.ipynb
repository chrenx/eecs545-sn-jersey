{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import unittest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a transforms for preparing the dataset\n",
    "transform = transforms.Compose([\n",
    "\n",
    "transforms.CenterCrop(26),\n",
    "# transforms.Resize((150,150)),\n",
    "# transforms.Resize((250, 250)),\n",
    "transforms.Pad(80),\n",
    "transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "transforms.Grayscale(1),\n",
    "transforms.RandomRotation(10),      \n",
    "transforms.RandomAffine(5),\n",
    "transforms.RandomPerspective(distortion_scale=0.65, p=0.8),\n",
    "transforms.ElasticTransform(),\n",
    "transforms.GaussianBlur(kernel_size=(3,3)),\n",
    "transforms.Resize((200,200)),\n",
    "\n",
    "# convert the image to a pytorch tensor\n",
    "transforms.ToTensor(), \n",
    "\n",
    "# normalise the images with mean and std of the dataset\n",
    "transforms.Normalize((0.1307,), (0.3081,)) \n",
    "])\n",
    "\n",
    "# Load the MNIST training, test datasets using `torchvision.datasets.MNIST` \n",
    "# using the transform defined above\n",
    "\n",
    "train_dataset = datasets.MNIST('./data_padding',train=True,transform=transform,download=True)\n",
    "test_dataset =  datasets.MNIST('./data_padding',train=False,transform=transform,download=True)\n",
    "\n",
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 32 and set shuffle=True for the training set\n",
    "\n",
    "train_dataloader = Data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=True) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 200, 200]),\n",
       " tensor(6),\n",
       " torch.Size([1, 200, 200]),\n",
       " <matplotlib.image.AxesImage at 0x7e4c383e7460>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAGhCAYAAADbf0s2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoGUlEQVR4nO3de3DV9Z3/8dfJ7eR6TjgJJ8lRwk2LKJcCapZpS7VkBeqgVrZVSke0FBYXsIWtZbMzSmV2DJVZ3XHLqjuD0B3FVWcERzrSQa51DYjBDONls4QNhEsuQsg5uZ5czuf3R3/51rNJwJQk55P4fMy8pznfz+f7zft8SfPy+/1+OLiMMUYAAFgoLtYNAADQF0IKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgrZiF1JYtWzRu3DglJyeroKBAH374YaxaAQBYKiYh9frrr2vdunXasGGDjh8/runTp2vevHmqq6uLRTsAAEu5YvEBswUFBbrtttv029/+VpIUiUQ0ZswYrVmzRv/wD/9w1f0jkYguXLigjIwMuVyuwW4XADDAjDFqbGxUIBBQXFzf10sJQ9iTJKm9vV2lpaUqKipytsXFxamwsFAlJSW97hMOhxUOh53X58+f18033zzovQIABtfZs2d1/fXX9zk+5Lf7Ll68qK6uLuXk5ERtz8nJUU1NTa/7FBcXy+v1OkVAAcDIkJGRccXxYbG6r6ioSMFg0KmzZ8/GuiUAwAC42iObIb/dl52drfj4eNXW1kZtr62tVW5ubq/7uN1uud3uoWgPAGCRIb+SSkpK0qxZs7Rv3z5nWyQS0b59+zR79uyhbgcAYLEhv5KSpHXr1mnp0qW69dZbdfvtt+tf/uVf1NzcrEceeSQW7QAALBWTkHrggQf0xRdf6Mknn1RNTY2++c1vas+ePT0WUwAAvt5i8vekrlUoFJLX6411GwCAaxQMBuXxePocHxar+wAAX0+EFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWgMeUsXFxbrtttuUkZEhv9+v++67T+Xl5VFz7rjjDrlcrqhauXLlQLcCABjmBjykDh06pFWrVunIkSPau3evOjo6dNddd6m5uTlq3vLly1VdXe3UM888M9CtAACGuYSBPuCePXuiXm/fvl1+v1+lpaWaM2eOsz01NVW5ubkD/e0BACPIoD+TCgaDkiSfzxe1/dVXX1V2dramTJmioqIitbS09HmMcDisUCgUVQCArwEziLq6uszdd99tvvWtb0Vtf+mll8yePXvMiRMnzCuvvGKuu+4684Mf/KDP42zYsMFIoiiKokZYBYPBK+bIoIbUypUrzdixY83Zs2evOG/fvn1GkqmoqOh1vK2tzQSDQafOnj0b8xNLURRFXXtdLaQG/JlUt9WrV2v37t06fPiwrr/++ivOLSgokCRVVFRo4sSJPcbdbrfcbveg9AkAsNeAh5QxRmvWrNHOnTt18OBBjR8//qr7lJWVSZLy8vIGuh0AwDA24CG1atUq7dixQ2+//bYyMjJUU1MjSfJ6vUpJSdGpU6e0Y8cOff/731dWVpZOnDihtWvXas6cOZo2bdpAtwMAGM7+0udNfVEf9x23bdtmjDGmqqrKzJkzx/h8PuN2u80NN9xgHn/88avel/yyYDAY8/uoFEVR1LXX1X73u/5/sAwroVBIXq831m0AAK5RMBiUx+Ppc5zP7gMAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYa8BD6te//rVcLldU3XTTTc54W1ubVq1apaysLKWnp2vRokWqra0d6DYAACPAoFxJ3XLLLaqurnbq/fffd8bWrl2rd955R2+++aYOHTqkCxcu6P777x+MNgAAw1zCoBw0IUG5ubk9tgeDQW3dulU7duzQ9773PUnStm3bNHnyZB05ckR/9Vd/NRjtAACGqUG5kjp58qQCgYAmTJigJUuWqKqqSpJUWlqqjo4OFRYWOnNvuukm5efnq6SkpM/jhcNhhUKhqAIAjHwDHlIFBQXavn279uzZoxdeeEGVlZX6zne+o8bGRtXU1CgpKUmZmZlR++Tk5KimpqbPYxYXF8vr9To1ZsyYgW4bAGChAb/dt2DBAufradOmqaCgQGPHjtUbb7yhlJSUv+iYRUVFWrdunfM6FAoRVADwNTDoS9AzMzP1jW98QxUVFcrNzVV7e7saGhqi5tTW1vb6DKub2+2Wx+OJKgDAyDfoIdXU1KRTp04pLy9Ps2bNUmJiovbt2+eMl5eXq6qqSrNnzx7sVgAAw8yA3+775S9/qYULF2rs2LG6cOGCNmzYoPj4eC1evFher1fLli3TunXr5PP55PF4tGbNGs2ePZuVfQCAHgY8pM6dO6fFixfr0qVLGj16tL797W/ryJEjGj16tCTpueeeU1xcnBYtWqRwOKx58+bp3/7t3wa6DQDACOAyxphYN9FfoVBIXq831m0AAK5RMBi84joDPrsPAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgLUIKAGAtQgoAYC1CCgBgrQEPqXHjxsnlcvWoVatWSZLuuOOOHmMrV64c6DYAACNAwkAf8NixY+rq6nJef/LJJ/rrv/5r/fCHP3S2LV++XBs3bnRep6amDnQbAIARYMBDavTo0VGvN23apIkTJ+q73/2usy01NVW5ublf+ZjhcFjhcNh5HQqFrr1RAID1BvWZVHt7u1555RX99Kc/lcvlcra/+uqrys7O1pQpU1RUVKSWlpYrHqe4uFher9epMWPGDGbbAABLuIwxZrAO/sYbb+jHP/6xqqqqFAgEJEn//u//rrFjxyoQCOjEiRNav369br/9dr311lt9Hqe3KymCCgCGv2AwKI/H0+f4oIbUvHnzlJSUpHfeeafPOfv379fcuXNVUVGhiRMnfqXjhkIheb3egWoTABAjVwupQbvdd+bMGb333nv62c9+dsV5BQUFkqSKiorBagUAMEwNWkht27ZNfr9fd9999xXnlZWVSZLy8vIGqxUAwDA14Kv7JCkSiWjbtm1aunSpEhL+/C1OnTqlHTt26Pvf/76ysrJ04sQJrV27VnPmzNG0adMGoxUAwHBmBsEf/vAHI8mUl5dHba+qqjJz5swxPp/PuN1uc8MNN5jHH3/cBIPBfh0/GAwaSRRFUdQwr6v9/h/UhRODhYUTADAyxGzhBAAA14qQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYq98hdfjwYS1cuFCBQEAul0u7du2KGjfG6Mknn1ReXp5SUlJUWFiokydPRs2pr6/XkiVL5PF4lJmZqWXLlqmpqema3ggw0sTFxSkuLk4ulyvWrQAx0++Qam5u1vTp07Vly5Zex5955hk9//zzevHFF3X06FGlpaVp3rx5amtrc+YsWbJEn376qfbu3avdu3fr8OHDWrFixV/+LoARxOVyKS4uTgkJCUpISHDCCvhaMtdAktm5c6fzOhKJmNzcXLN582ZnW0NDg3G73ea1114zxhjz2WefGUnm2LFjzpx3333XuFwuc/78+V6/T1tbmwkGg06dPXvWSKKoEVVxcXEmPj7epKWlGa/XawKBgMnLyzNer9ekpaWZ+Ph443K5+tzf5XJdcZyibKxgMHjFnBnQ/zyrrKxUTU2NCgsLnW1er1cFBQUqKSmRJJWUlCgzM1O33nqrM6ewsFBxcXE6evRor8ctLi6W1+t1asyYMQPZNhBzLpdLSUlJSk1N1XXXXacxY8bI5/PJ4/E4V1NX2vfLtwa5PYiRJGEgD1ZTUyNJysnJidqek5PjjNXU1Mjv90c3kZAgn8/nzPm/ioqKtG7dOud1KBQiqDBixMXFKT4+Xj6fT5mZmVq4cKEyMzP1+eef64svvlBTU5MikUiP/brDyeVyKSHhT/9XNsYoEomos7NTf7rZAQxvAxpSg8Xtdsvtdse6DWBQxMfHKzExUenp6crMzFRWVpYyMjIUDofV0tKijo4OdXZ2SpJzRZWQkKD4+HglJyc7z64kKRwOq6OjQy0tLYpEIr2GGzCcDGhI5ebmSpJqa2uVl5fnbK+trdU3v/lNZ05dXV3Ufp2dnaqvr3f2B74Oum/Nud1upaSkyO/3Ky8vT3l5eXK73aqrq1Ntba2am5udK6P4+HglJCQoLS1NKSkpGjVqlJKTk5WUlCRjjC5duqTW1lZ1dnaqs7NTHR0dXFFhWBvQkBo/frxyc3O1b98+J5RCoZCOHj2qRx99VJI0e/ZsNTQ0qLS0VLNmzZIk7d+/X5FIRAUFBQPZDmCt7lt0SUlJ8vv98vl8mjBhgnJyctTQ0KBwOKzLly87t/ri4+OVkpIin88nv98vv9/vXHUlJyero6NDra2t+p//+R/V19crGAzKGKOOjo5Yv1XgmvQ7pJqamlRRUeG8rqysVFlZmXw+n/Lz8/WLX/xC//RP/6Qbb7xR48eP1xNPPKFAIKD77rtPkjR58mTNnz9fy5cv14svvqiOjg6tXr1aDz74oAKBwIC9McBm3Qsl0tPTlZeXp/z8fE2aNEmjR4/W+fPnVVdXp/r6ejU2Nqqrq0spKSnKzMzUhAkTdPPNN2vChAnKy8uT3+9XcnKyLl68qMuXLyscDishIUHnzp1TV1dXrN8mcM36HVIfffSR7rzzTud194KGpUuXavv27frVr36l5uZmrVixQg0NDfr2t7+tPXv2KDk52dnn1Vdf1erVqzV37lzFxcVp0aJFev755wfg7QD2617w0B1S2dnZCgQCysjIUEJCgurq6lRdXa2Ojg4lJCQoOztbfr9fU6dOVXZ2tkaPHu3c2ktOTnaeV3U/m+peDcgqP4wE/Q6pO+6444r3uF0ulzZu3KiNGzf2Ocfn82nHjh39/dbAiBEXF6fExESlpKTI4/E4t+1cLpcaGhpUX1/v3ObLzMxUXl6ebrnlFmcRUVtbmxobG9Xc3By17Dw+Pl7x8fEsRceIMSxW9wEjxZdv840ePdr5+1But1s1NTVqbW1VXV2dmpubNWbMGKWlpWnSpEnyeDyS5NwC/PLfjUpNTVVzc7MaGxt1+vRpffHFF2ptbVV7ezuLJjDsEVLAEHK5XM7S8fT0dPl8PucKqrGxUQ0NDWpra1NXV5ezOGLMmDGKj49XJBJRW1ubgsGgkpKSlJiYqObmZnV0dOjy5csKhULO/3Z0dLD8HCMCIQUMke7nUBkZGRozZozGjx+vm2++WV1dXQoGg6qqqtLFixeVmpqqrKwszZw5U16vVzk5Obp48aI+//xzGWPkdruVmZmptLQ057bfyZMnVV9frzNnzqilpUWtra2EFEYEPrUSGALdz4gSEhLkdruVkZHhLCFPSUmRJLW3tyscDislJcUJJ7/fL6/Xq7S0NMXHx8vtdis9PV0pKSlKSkpSOBxWU1OTGhoaFAqF1NLSonA4rEgkwq0+jAhcSQFDpPv50ahRozRu3DhNnDhRkyZN0rlz51RbW6u0tDQ1NzcrPz9ffr9fU6ZMUXp6uhISEpSenq5IJKKkpCS53W7V19crFAqptrZWtbW1OnfunJqamtTc3Kyuri4CCiMGIQUMoe4rqu6FD/Hx8UpNTZXX61V2drYkKSsrS6NGjVJGRobS09OVlJSkSCSi0aNHq7OzU11dXWptbVVDQ4MuX76shoYGrqAwYhFSwBDq/gDYcDjs1KhRo5wVfo2NjfL7/fJ4PMrJyVFaWprS0tLk9XqVmJio8+fPq6qqSmfOnFFlZaUqKip0+fJlBYNBJ8AIKYwkhBQwBLrDqb29XaFQSDU1NUpNTVVmZqY8Ho/S09PlcrmclX5dXV2qr69XU1OT4uPjFQqFdPbsWZ07d07/+7//q6qqKlVXVysUCqm1tVVdXV1cRWFEIqSAIRKJRNTc3KxIJKLy8nI1NTWpqalJY8eOVSAQUFpamlJTU2WMUWtrq86cOaOuri61tLTo8uXLOn36tM6cOaNTp06prq5ODQ0NzofJ8hFIGKkIKWCIGGPU1dWlcDjsfABsa2urqqurnSXl3c+fIpGI4uLinKuvlpYW5xlU91/obWtrU2dnJ0vNMaIRUsAQ6n4e1dHRocbGRtXW1jp/MTc9PV2JiYnOX+Ztb29XJBJxrpKMMc5VE7f38HVBSAEx0B0wX766CofDio+Pd/7tqO5FEN1B1P31lwsY6QgpIEa6g6j7Sqm9vV0ulysqlICvO0IKsARXR0BPfCwSAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWv0OqcOHD2vhwoUKBAJyuVzatWuXM9bR0aH169dr6tSpSktLUyAQ0EMPPaQLFy5EHWPcuHFyuVxRtWnTpmt+MwCAkaXfIdXc3Kzp06dry5YtPcZaWlp0/PhxPfHEEzp+/LjeeustlZeX65577ukxd+PGjaqurnZqzZo1f9k7AACMWAn93WHBggVasGBBr2Ner1d79+6N2vbb3/5Wt99+u6qqqpSfn+9sz8jIUG5ubn+/PQDga2TQn0kFg0G5XC5lZmZGbd+0aZOysrI0Y8YMbd68WZ2dnX0eIxwOKxQKRRUAYOTr95VUf7S1tWn9+vVavHixPB6Ps/2xxx7TzJkz5fP59MEHH6ioqEjV1dV69tlnez1OcXGxnnrqqcFsFQBgI3MNJJmdO3f2Otbe3m4WLlxoZsyYYYLB4BWPs3XrVpOQkGDa2tp6HW9razPBYNCps2fPGkkURVHUMK+r5cOgXEl1dHToRz/6kc6cOaP9+/dHXUX1pqCgQJ2dnTp9+rQmTZrUY9ztdsvtdg9GqwAAiw14SHUH1MmTJ3XgwAFlZWVddZ+ysjLFxcXJ7/cPdDsAgGGs3yHV1NSkiooK53VlZaXKysrk8/mUl5env/mbv9Hx48e1e/dudXV1qaamRpLk8/mUlJSkkpISHT16VHfeeacyMjJUUlKitWvX6ic/+YlGjRo1cO8MADD8faWHT19y4MCBXu8rLl261FRWVvZ53/HAgQPGGGNKS0tNQUGB8Xq9Jjk52UyePNk8/fTTfT6P6k0wGIz5fVSKoijq2utqz6RcxhijYSYUCsnr9ca6DQDANQoGg1dct8Bn9wEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCsRUgBAKxFSAEArEVIAQCs1e+QOnz4sBYuXKhAICCXy6Vdu3ZFjT/88MNyuVxRNX/+/Kg59fX1WrJkiTwejzIzM7Vs2TI1NTVd0xsBAIw8/Q6p5uZmTZ8+XVu2bOlzzvz581VdXe3Ua6+9FjW+ZMkSffrpp9q7d692796tw4cPa8WKFf3vHgAwsplrIMns3LkzatvSpUvNvffe2+c+n332mZFkjh075mx79913jcvlMufPn/9K3zcYDBpJFEVR1DCvYDB4xd/3g/JM6uDBg/L7/Zo0aZIeffRRXbp0yRkrKSlRZmambr31VmdbYWGh4uLidPTo0V6PFw6HFQqFogoAMPINeEjNnz9f//Ef/6F9+/bpN7/5jQ4dOqQFCxaoq6tLklRTUyO/3x+1T0JCgnw+n2pqano9ZnFxsbxer1NjxowZ6LYBABZKGOgDPvjgg87XU6dO1bRp0zRx4kQdPHhQc+fO/YuOWVRUpHXr1jmvQ6EQQQUAXwODvgR9woQJys7OVkVFhSQpNzdXdXV1UXM6OztVX1+v3NzcXo/hdrvl8XiiCgAw8g16SJ07d06XLl1SXl6eJGn27NlqaGhQaWmpM2f//v2KRCIqKCgY7HYAAMNIv2/3NTU1OVdFklRZWamysjL5fD75fD499dRTWrRokXJzc3Xq1Cn96le/0g033KB58+ZJkiZPnqz58+dr+fLlevHFF9XR0aHVq1frwQcfVCAQGLh3BgAY/r7Smu8vOXDgQK/LCJcuXWpaWlrMXXfdZUaPHm0SExPN2LFjzfLly01NTU3UMS5dumQWL15s0tPTjcfjMY888ohpbGz8yj2wBJ2iKGpk1NWWoLuMMUbDTCgUktfrjXUbAIBrFAwGr7jOgM/uAwBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFir3yF1+PBhLVy4UIFAQC6XS7t27Yoad7lcvdbmzZudOePGjesxvmnTpmt+MwCAkaXfIdXc3Kzp06dry5YtvY5XV1dH1csvvyyXy6VFixZFzdu4cWPUvDVr1vxl7wAAMGIl9HeHBQsWaMGCBX2O5+bmRr1+++23deedd2rChAlR2zMyMnrMBQDgywb1mVRtba1+//vfa9myZT3GNm3apKysLM2YMUObN29WZ2dnn8cJh8MKhUJRBQAY+fp9JdUfv/vd75SRkaH7778/avtjjz2mmTNnyufz6YMPPlBRUZGqq6v17LPP9nqc4uJiPfXUU4PZKgDARuYaSDI7d+7sc3zSpElm9erVVz3O1q1bTUJCgmlra+t1vK2tzQSDQafOnj1rJFEURVHDvILB4BXzYdCupP74xz+qvLxcr7/++lXnFhQUqLOzU6dPn9akSZN6jLvdbrnd7sFoEwBgsUF7JrV161bNmjVL06dPv+rcsrIyxcXFye/3D1Y7AIBhqN9XUk1NTaqoqHBeV1ZWqqysTD6fT/n5+ZKkUCikN998U//8z//cY/+SkhIdPXpUd955pzIyMlRSUqK1a9fqJz/5iUaNGnUNbwUAMOJc9YHR/3HgwIFe7ysuXbrUmfPSSy+ZlJQU09DQ0GP/0tJSU1BQYLxer0lOTjaTJ082Tz/9dJ/Po3oTDAZjfh+VoiiKuva62jMplzHGaJgJhULyer2xbgMAcI2CwaA8Hk+f43x2HwDAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWoQUAMBahBQAwFqEFADAWsMypIwxsW4BADAArvb7fFiGVGNjY6xbAAAMgKv9PneZYXhZEolEVF5erptvvllnz56Vx+OJdUtfWSgU0pgxY+h7CA3X3ul7aNH30DLGqLGxUYFAQHFxfV8vJQxhTwMmLi5O1113nSTJ4/EMqz+YbvQ99IZr7/Q9tOh76Hi93qvOGZa3+wAAXw+EFADAWsM2pNxutzZs2CC32x3rVvqFvofecO2dvocWfdtpWC6cAAB8PQzbKykAwMhHSAEArEVIAQCsRUgBAKxFSAEArDVsQ2rLli0aN26ckpOTVVBQoA8//DDWLTmKi4t12223KSMjQ36/X/fdd5/Ky8uj5txxxx1yuVxRtXLlyhh1/Ge//vWve/R10003OeNtbW1atWqVsrKylJ6erkWLFqm2tjaGHf/JuHHjevTtcrm0atUqSfac78OHD2vhwoUKBAJyuVzatWtX1LgxRk8++aTy8vKUkpKiwsJCnTx5MmpOfX29lixZIo/Ho8zMTC1btkxNTU0x67ujo0Pr16/X1KlTlZaWpkAgoIceekgXLlyIOkZvf0abNm0a1L6v1rskPfzwwz36mj9/ftQc2865pF5/3l0ulzZv3uzMidU5H0jDMqRef/11rVu3Ths2bNDx48c1ffp0zZs3T3V1dbFuTZJ06NAhrVq1SkeOHNHevXvV0dGhu+66S83NzVHzli9frurqaqeeeeaZGHUc7ZZbbonq6/3333fG1q5dq3feeUdvvvmmDh06pAsXLuj++++PYbd/cuzYsaie9+7dK0n64Q9/6Myx4Xw3Nzdr+vTp2rJlS6/jzzzzjJ5//nm9+OKLOnr0qNLS0jRv3jy1tbU5c5YsWaJPP/1Ue/fu1e7du3X48GGtWLEiZn23tLTo+PHjeuKJJ3T8+HG99dZbKi8v1z333NNj7saNG6P+DNasWTOofV+t927z58+P6uu1116LGrftnEuK6re6ulovv/yyXC6XFi1aFDUvFud8QJlh6PbbbzerVq1yXnd1dZlAIGCKi4tj2FXf6urqjCRz6NAhZ9t3v/td8/Of/zx2TfVhw4YNZvr06b2ONTQ0mMTERPPmm2862z7//HMjyZSUlAxRh1/Nz3/+czNx4kQTiUSMMXaeb0lm586dzutIJGJyc3PN5s2bnW0NDQ3G7Xab1157zRhjzGeffWYkmWPHjjlz3n33XeNyucz58+dj0ndvPvzwQyPJnDlzxtk2duxY89xzzw1uc1fRW+9Lly419957b5/7DJdzfu+995rvfe97UdtsOOfXathdSbW3t6u0tFSFhYXOtri4OBUWFqqkpCSGnfUtGAxKknw+X9T2V199VdnZ2ZoyZYqKiorU0tISi/Z6OHnypAKBgCZMmKAlS5aoqqpKklRaWqqOjo6oc3/TTTcpPz/fqnPf3t6uV155RT/96U/lcrmc7bae726VlZWqqamJOr9er1cFBQXO+S0pKVFmZqZuvfVWZ05hYaHi4uJ09OjRIe+5L8FgUC6XS5mZmVHbN23apKysLM2YMUObN29WZ2dnbBr8Pw4ePCi/369Jkybp0Ucf1aVLl5yx4XDOa2tr9fvf/17Lli3rMWbrOf+qht2noF+8eFFdXV3KycmJ2p6Tk6P//u//jlFXfYtEIvrFL36hb33rW5oyZYqz/cc//rHGjh2rQCCgEydOaP369SovL9dbb70Vw26lgoICbd++XZMmTVJ1dbWeeuopfec739Enn3yimpoaJSUl9fjFk5OTo5qamtg03Itdu3apoaFBDz/8sLPN1vP9Zd3nsLef7e6xmpoa+f3+qPGEhAT5fD5r/gza2tq0fv16LV68OOpTuR977DHNnDlTPp9PH3zwgYqKilRdXa1nn302ht3+6Vbf/fffr/Hjx+vUqVP6x3/8Ry1YsEAlJSWKj48fFuf8d7/7nTIyMnrcerf1nPfHsAup4WbVqlX65JNPop7rSIq6nz116lTl5eVp7ty5OnXqlCZOnDjUbToWLFjgfD1t2jQVFBRo7NixeuONN5SSkhKzvvpj69atWrBggQKBgLPN1vM90nR0dOhHP/qRjDF64YUXosbWrVvnfD1t2jQlJSXpb//2b1VcXBzTz5178MEHna+nTp2qadOmaeLEiTp48KDmzp0bs7764+WXX9aSJUuUnJwctd3Wc94fw+52X3Z2tuLj43usKKutrVVubm6Muurd6tWrtXv3bh04cEDXX3/9FecWFBRIkioqKoaita8sMzNT3/jGN1RRUaHc3Fy1t7eroaEhao5N5/7MmTN677339LOf/eyK82w8393n8Eo/27m5uT0WCHV2dqq+vj7mfwbdAXXmzBnt3bv3qv+2UUFBgTo7O3X69OmhafArmjBhgrKzs52fDZvPuST98Y9/VHl5+VV/5iV7z/mVDLuQSkpK0qxZs7Rv3z5nWyQS0b59+zR79uwYdvZnxhitXr1aO3fu1P79+zV+/Pir7lNWViZJysvLG+Tu+qepqUmnTp1SXl6eZs2apcTExKhzX15erqqqKmvO/bZt2+T3+3X33XdfcZ6N53v8+PHKzc2NOr+hUEhHjx51zu/s2bPV0NCg0tJSZ87+/fsViUSc4I2F7oA6efKk3nvvPWVlZV11n7KyMsXFxfW4lRZr586d06VLl5yfDVvPebetW7dq1qxZmj59+lXn2nrOryjWKzf+Ev/5n/9p3G632b59u/nss8/MihUrTGZmpqmpqYl1a8YYYx599FHj9XrNwYMHTXV1tVMtLS3GGGMqKirMxo0bzUcffWQqKyvN22+/bSZMmGDmzJkT486N+fu//3tz8OBBU1lZaf7rv/7LFBYWmuzsbFNXV2eMMWblypUmPz/f7N+/33z00Udm9uzZZvbs2THu+k+6urpMfn6+Wb9+fdR2m853Y2Oj+fjjj83HH39sJJlnn33WfPzxx84quE2bNpnMzEzz9ttvmxMnTph7773XjB8/3rS2tjrHmD9/vpkxY4Y5evSoef/9982NN95oFi9eHLO+29vbzT333GOuv/56U1ZWFvUzHw6HjTHGfPDBB+a5554zZWVl5tSpU+aVV14xo0ePNg899NCg9n213hsbG80vf/lLU1JSYiorK817771nZs6caW688UbT1tbmHMO2c94tGAya1NRU88ILL/TYP5bnfCANy5Ayxph//dd/Nfn5+SYpKcncfvvt5siRI7FuySGp19q2bZsxxpiqqiozZ84c4/P5jNvtNjfccIN5/PHHTTAYjG3jxpgHHnjA5OXlmaSkJHPdddeZBx54wFRUVDjjra2t5u/+7u/MqFGjTGpqqvnBD35gqqurY9jxn/3hD38wkkx5eXnUdpvO94EDB3r92Vi6dKkx5k/L0J944gmTk5Nj3G63mTt3bo/3c+nSJbN48WKTnp5uPB6PeeSRR0xjY2PM+q6srOzzZ/7AgQPGGGNKS0tNQUGB8Xq9Jjk52UyePNk8/fTTUUEQi95bWlrMXXfdZUaPHm0SExPN2LFjzfLly3v8B69t57zbSy+9ZFJSUkxDQ0OP/WN5zgcS/54UAMBaw+6ZFADg64OQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBYi5ACAFiLkAIAWIuQAgBY6/8BZIUhCk0ebFQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a= next(iter(train_dataloader))\n",
    "b = a[0][0].detach().clone()\n",
    "a[0].shape, a[1][0], b.shape, plt.imshow(b.permute(1,2,0), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #! 210 input\n",
    "        # define a conv layer with output channels as 16, kernel size of 3 and stride of 1\n",
    "        self.conv11 = nn.Conv2d(1, 16, 6, 2) # Input = 1x28x28  Output = 16x26x26\n",
    "        self.conv12 = nn.Conv2d(1, 16, 10, 2) # Input = 1x28x28  Output = 16x24x24\n",
    "        self.conv13 = nn.Conv2d(1, 16, 14, 2) # Input = 1x28x28  Output = 16x22x22\n",
    "        self.conv14 = nn.Conv2d(1, 16, 18, 2) # Input = 1x28x28  Output = 16x20x20\n",
    "\n",
    "        # define a conv layer with output channels as 32, kernel size of 3 and stride of 1\n",
    "        self.conv21 = nn.Conv2d(16, 32, 3, 2) # Input = 16x26x26 Output = 32x24x24\n",
    "        self.conv22 = nn.Conv2d(16, 32, 5, 2) # Input = 16x24x24 Output = 32x20x20\n",
    "        self.conv23 = nn.Conv2d(16, 32, 7, 2) # Input = 16x22x22 Output = 32x16x16\n",
    "        self.conv24 = nn.Conv2d(16, 32, 9, 2) # Input = 16x20x20  Output = 32x12x12\n",
    "\n",
    "        # define a conv layer with output channels as 64, kernel size of 3 and stride of 1\n",
    "        self.conv31 = nn.Conv2d(32, 64, 3, 2) # Input = 32x24x24 Output = 64x22x22\n",
    "        self.conv32 = nn.Conv2d(32, 64, 5, 2) # Input = 32x20x20 Output = 64x16x16\n",
    "        self.conv33 = nn.Conv2d(32, 64, 7, 2) # Input = 32x16x16 Output = 64x10x10\n",
    "        self.conv34 = nn.Conv2d(32, 64, 9, 2) # Input = 32x12x12 Output = 64x4x4\n",
    "\n",
    "        self.conv41 = nn.Conv2d(64, 128, 3, 1) # Input = 32x24x24 Output = 64x22x22\n",
    "        self.conv42 = nn.Conv2d(64, 128, 5, 1) # Input = 32x20x20 Output = 64x16x16\n",
    "        self.conv43 = nn.Conv2d(64, 128, 7, 1) # Input = 32x16x16 Output = 64x10x10\n",
    "        self.conv44 = nn.Conv2d(64, 128, 9, 1) # Input = 32x12x12 Output = 64x4x4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # define a max pooling layer with kernel size 2\n",
    "        self.maxpool = nn.MaxPool2d(2) # Output = 64x11x11\n",
    "        #self.maxpool1 = nn.MaxPool2d(1)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        # define dropout layer with a probability of 0.5\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # define a linear(dense) layer with 128 output features\n",
    "        # self.fc11 = nn.Linear(64*11*11, 256)\n",
    "        self.fc11 = nn.Linear(128*10*10, 256)\n",
    "        # self.fc12 = nn.Linear(64*8*8, 256)      # after maxpooling 2x2\n",
    "        self.fc12 = nn.Linear(128*8*8, 256)\n",
    "        # self.fc13 = nn.Linear(64*5*5, 256)\n",
    "        self.fc13 = nn.Linear(128*6*6, 256)\n",
    "        # self.fc14 = nn.Linear(64*2*2, 256)\n",
    "        self.fc14 = nn.Linear(128*4*4, 256)\n",
    "\n",
    "        # define a linear(dense) layer with output features corresponding to the number of classes in the dataset\n",
    "        self.fc21 = nn.Linear(256, 128)\n",
    "        self.fc22 = nn.Linear(256, 128)\n",
    "        self.fc23 = nn.Linear(256, 128)\n",
    "        self.fc24 = nn.Linear(256, 128)\n",
    "\n",
    "        self.fc33 = nn.Linear(128*4,10)\n",
    "        #self.fc33 = nn.Linear(64*3,10)\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and \n",
    "        # write the forward pass, after each of conv1, conv2, conv3 and fc1 use a relu activation. \n",
    "\n",
    "        x = F.relu(self.conv11(inp))\n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.conv31(x))\n",
    "        x = F.relu(self.maxpool(self.conv41(x)))\n",
    "        #print(x.shape)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = x.view(-1,64*11*11)\n",
    "        # print(\"++1++ \", x.shape)\n",
    "        x = x.view(-1,128*10*10)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc11(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc21(x)\n",
    "        # print(\"++1++ \", x.shape)\n",
    "\n",
    "        y = F.relu(self.conv12(inp))\n",
    "        y = F.relu(self.conv22(y))\n",
    "        y = F.relu(self.conv32(y))\n",
    "        y = F.relu(self.maxpool(self.conv42(y)))\n",
    "        #x = torch.flatten(x, 1)\n",
    "\n",
    "        \n",
    "        # y = y.view(-1,64*8*8)\n",
    "        y = y.view(-1,128*8*8)\n",
    "        y = self.dropout1(y)\n",
    "        y = F.relu(self.fc12(y))\n",
    "        y = self.dropout2(y)\n",
    "        y = self.fc22(y)\n",
    "        # print(\"++123++ \", y.shape)\n",
    "\n",
    "        z = F.relu(self.conv13(inp))\n",
    "        z = F.relu(self.conv23(z))\n",
    "        z = F.relu(self.conv33(z))\n",
    "        z = F.relu(self.maxpool(self.conv43(z)))\n",
    "        #x = torch.flatten(x, 1)\n",
    "\n",
    "        \n",
    "        # z = z.view(-1,64*5*5)\n",
    "        z = z.view(-1,128*6*6)\n",
    "        z = self.dropout1(z)\n",
    "        z = F.relu(self.fc13(z))\n",
    "        z = self.dropout2(z)\n",
    "        z = self.fc23(z)\n",
    "        # print(\"+++++435+ \", z.shape)\n",
    "\n",
    "        # print(\"inp \", inp.shape)\n",
    "        ze = F.relu(self.conv14(inp))\n",
    "        ze = F.relu(self.conv24(ze))\n",
    "        ze = F.relu(self.conv34(ze))\n",
    "        ze = F.relu(self.maxpool(self.conv44(ze)))\n",
    "        #x = torch.flatten(x, 1)\n",
    "\n",
    "        # print(\"kkk \", ze.shape)\n",
    "        # ze = ze.view(-1,64*2*2)\n",
    "        ze = ze.view(-1,128*4*4)\n",
    "        ze = self.dropout1(ze)\n",
    "        ze = F.relu(self.fc14(ze))\n",
    "        ze = self.dropout2(ze)\n",
    "        ze = self.fc24(ze)\n",
    "        # print(\"+++87+++ \", ze.shape)\n",
    "\n",
    "        out_f = torch.cat((x, y, z, ze), dim=1)\n",
    "        #out_f1 = torch.cat((out_f, ze), dim=1)\n",
    "        out = self.fc33(out_f)\n",
    "\n",
    "        output = F.log_softmax(out, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv11): Conv2d(1, 16, kernel_size=(6, 6), stride=(2, 2))\n",
       "  (conv12): Conv2d(1, 16, kernel_size=(10, 10), stride=(2, 2))\n",
       "  (conv13): Conv2d(1, 16, kernel_size=(14, 14), stride=(2, 2))\n",
       "  (conv14): Conv2d(1, 16, kernel_size=(18, 18), stride=(2, 2))\n",
       "  (conv21): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv22): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv23): Conv2d(16, 32, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv24): Conv2d(16, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "  (conv31): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv32): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv33): Conv2d(32, 64, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv34): Conv2d(32, 64, kernel_size=(9, 9), stride=(2, 2))\n",
       "  (conv41): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv42): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv43): Conv2d(64, 128, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv44): Conv2d(64, 128, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc11): Linear(in_features=12800, out_features=256, bias=True)\n",
       "  (fc12): Linear(in_features=8192, out_features=256, bias=True)\n",
       "  (fc13): Linear(in_features=4608, out_features=256, bias=True)\n",
       "  (fc14): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (fc21): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc22): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc23): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc24): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc33): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1 = []\n",
    "losses_2 = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send the image, target to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # flush out the gradients stored in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # pass the image to the model and assign the output to variable named output\n",
    "        output = model(data)\n",
    "        # calculate the loss (use nll_loss in pytorch)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # do a backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            losses_1.append(loss.item())\n",
    "            losses_2.append(100. * batch_idx / len(train_loader))\n",
    "\n",
    "accuracy = []\n",
    "avg_loss = []\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "          \n",
    "            # send the image, target to the device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # pass the image to the model and assign the output to variable named output\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "          \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    avg_loss.append(test_loss)\n",
    "    accuracy.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Net().to(device)\n",
    "learning_rate = []\n",
    "def adjust_learning_rate(optimizer, iter, each):\n",
    "    # sets the learning rate to the initial LR decayed by 0.1 every 'each' iterations\n",
    "    lr = 0.001 * (0.95 ** (iter // each))\n",
    "    state_dict = optimizer.state_dict()\n",
    "    for param_group in state_dict['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.load_state_dict(state_dict)\n",
    "    print(\"Learning rate = \",lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(0,120):\n",
    "  lr = adjust_learning_rate(optimizer, epoch, 1.616)\n",
    "  learning_rate.append(lr)\n",
    "  train(model, device, train_dataloader, optimizer, epoch)\n",
    "  test(model, device, test_dataloader)\n",
    "  if (epoch + 1) % 10 == 0:\n",
    "    torch.save(\n",
    "        {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, \n",
    "        f\"mnist_model_padding_ckpt_E{epoch}.pth\"\n",
    "    )\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, \n",
    "    \"mnist_model_ckpt.pth\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
