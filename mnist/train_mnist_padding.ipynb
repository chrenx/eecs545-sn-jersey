{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import unittest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a transforms for preparing the dataset\n",
    "transform = transforms.Compose([\n",
    "\n",
    "transforms.CenterCrop(26),\n",
    "# transforms.Resize((150,150)),\n",
    "# transforms.Resize((250, 250)),\n",
    "transforms.Pad(80),\n",
    "transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "transforms.Grayscale(1),\n",
    "transforms.RandomRotation(10),      \n",
    "transforms.RandomAffine(5),\n",
    "transforms.RandomPerspective(distortion_scale=0.65, p=0.8),\n",
    "transforms.ElasticTransform(),\n",
    "transforms.GaussianBlur(kernel_size=(3,3)),\n",
    "transforms.Resize((200,200)),\n",
    "\n",
    "# convert the image to a pytorch tensor\n",
    "transforms.ToTensor(), \n",
    "\n",
    "# normalise the images with mean and std of the dataset\n",
    "transforms.Normalize((0.1307,), (0.3081,)) \n",
    "])\n",
    "\n",
    "# Load the MNIST training, test datasets using `torchvision.datasets.MNIST` \n",
    "# using the transform defined above\n",
    "\n",
    "train_dataset = datasets.MNIST('./data_padding',train=True,transform=transform,download=True)\n",
    "test_dataset =  datasets.MNIST('./data_padding',train=False,transform=transform,download=True)\n",
    "\n",
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 32 and set shuffle=True for the training set\n",
    "\n",
    "train_dataloader = Data.DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = Data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 1, 200, 200]),\n",
       " tensor(8),\n",
       " torch.Size([1, 200, 200]),\n",
       " <matplotlib.image.AxesImage at 0x73c0d33ab490>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a= next(iter(train_dataloader))\n",
    "b = a[0][10].detach().clone()\n",
    "a[0].shape, a[1][10], b.shape, plt.imshow(b.permute(1,2,0), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #! 210 input\n",
    "        # define a conv layer with output channels as 16, kernel size of 3 and stride of 1\n",
    "        self.conv11 = nn.Conv2d(1, 16, 3, 2) # Input = 1x28x28  Output = 16x26x26\n",
    "        self.conv12 = nn.Conv2d(1, 16, 5, 2) # Input = 1x28x28  Output = 16x24x24\n",
    "        self.conv13 = nn.Conv2d(1, 16, 7, 2) # Input = 1x28x28  Output = 16x22x22\n",
    "        self.conv14 = nn.Conv2d(1, 16, 9, 2) # Input = 1x28x28  Output = 16x20x20\n",
    "\n",
    "        # define a conv layer with output channels as 32, kernel size of 3 and stride of 1\n",
    "        self.conv21 = nn.Conv2d(16, 32, 3, 2) # Input = 16x26x26 Output = 32x24x24\n",
    "        self.conv22 = nn.Conv2d(16, 32, 5, 2) # Input = 16x24x24 Output = 32x20x20\n",
    "        self.conv23 = nn.Conv2d(16, 32, 7, 2) # Input = 16x22x22 Output = 32x16x16\n",
    "        self.conv24 = nn.Conv2d(16, 32, 9, 2) # Input = 16x20x20  Output = 32x12x12\n",
    "\n",
    "        # define a conv layer with output channels as 64, kernel size of 3 and stride of 1\n",
    "        self.conv31 = nn.Conv2d(32, 64, 3, 1) # Input = 32x24x24 Output = 64x22x22\n",
    "        self.conv32 = nn.Conv2d(32, 64, 5, 1) # Input = 32x20x20 Output = 64x16x16\n",
    "        self.conv33 = nn.Conv2d(32, 64, 7, 1) # Input = 32x16x16 Output = 64x10x10\n",
    "        self.conv34 = nn.Conv2d(32, 64, 9, 1) # Input = 32x12x12 Output = 64x4x4\n",
    "\n",
    "\n",
    "        # define a max pooling layer with kernel size 2\n",
    "        self.maxpool = nn.MaxPool2d(2) # Output = 64x11x11\n",
    "        #self.maxpool1 = nn.MaxPool2d(1)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        # define dropout layer with a probability of 0.5\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # define a linear(dense) layer with 128 output features\n",
    "        # self.fc11 = nn.Linear(64*11*11, 256)\n",
    "        self.fc11 = nn.Linear(64*11*11, 256)\n",
    "        # self.fc12 = nn.Linear(64*8*8, 256)      # after maxpooling 2x2\n",
    "        self.fc12 = nn.Linear(64*9*9, 256)\n",
    "        # self.fc13 = nn.Linear(64*5*5, 256)\n",
    "        self.fc13 = nn.Linear(64*7*7, 256)\n",
    "        # self.fc14 = nn.Linear(64*2*2, 256)\n",
    "        self.fc14 = nn.Linear(64*5*5, 256)\n",
    "\n",
    "        # define a linear(dense) layer with output features corresponding to the number of classes in the dataset\n",
    "        self.fc21 = nn.Linear(256, 128)\n",
    "        self.fc22 = nn.Linear(256, 128)\n",
    "        self.fc23 = nn.Linear(256, 128)\n",
    "        self.fc24 = nn.Linear(256, 128)\n",
    "\n",
    "        self.fc33 = nn.Linear(128*4,10)\n",
    "        #self.fc33 = nn.Linear(64*3,10)\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and \n",
    "        # write the forward pass, after each of conv1, conv2, conv3 and fc1 use a relu activation. \n",
    "\n",
    "        x = F.relu(self.conv11(inp))\n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.maxpool(self.conv31(x)))\n",
    "        #print(x.shape)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        # x = x.view(-1,64*11*11)\n",
    "        # print(\"++1++ \", x.shape)\n",
    "        x = x.view(-1,64*11*11)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc11(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc21(x)\n",
    "        # print(\"++1++ \", x.shape)\n",
    "\n",
    "        y = F.relu(self.conv12(inp))\n",
    "        y = F.relu(self.conv22(y))\n",
    "        y = F.relu(self.maxpool(self.conv32(y)))\n",
    "        #x = torch.flatten(x, 1)\n",
    "\n",
    "        \n",
    "        # y = y.view(-1,64*8*8)\n",
    "        y = y.view(-1,64*9*9)\n",
    "        y = self.dropout1(y)\n",
    "        y = F.relu(self.fc12(y))\n",
    "        y = self.dropout2(y)\n",
    "        y = self.fc22(y)\n",
    "        # print(\"++123++ \", y.shape)\n",
    "\n",
    "        z = F.relu(self.conv13(inp))\n",
    "        z = F.relu(self.conv23(z))\n",
    "        z = F.relu(self.maxpool(self.conv33(z)))\n",
    "        #x = torch.flatten(x, 1)\n",
    "\n",
    "        \n",
    "        # z = z.view(-1,64*5*5)\n",
    "        z = z.view(-1,64*7*7)\n",
    "        z = self.dropout1(z)\n",
    "        z = F.relu(self.fc13(z))\n",
    "        z = self.dropout2(z)\n",
    "        z = self.fc23(z)\n",
    "        # print(\"+++++435+ \", z.shape)\n",
    "\n",
    "        # print(\"inp \", inp.shape)\n",
    "        ze = F.relu(self.conv14(inp))\n",
    "        ze = F.relu(self.conv24(ze))\n",
    "        ze = F.relu(self.maxpool(self.conv34(ze)))\n",
    "        #x = torch.flatten(x, 1)\n",
    "\n",
    "        # print(\"kkk \", ze.shape)\n",
    "        # ze = ze.view(-1,64*2*2)\n",
    "        ze = ze.view(-1,64*5*5)\n",
    "        ze = self.dropout1(ze)\n",
    "        ze = F.relu(self.fc14(ze))\n",
    "        ze = self.dropout2(ze)\n",
    "        ze = self.fc24(ze)\n",
    "        # print(\"+++87+++ \", ze.shape)\n",
    "\n",
    "        out_f = torch.cat((x, y, z, ze), dim=1)\n",
    "        #out_f1 = torch.cat((out_f, ze), dim=1)\n",
    "        out = self.fc33(out_f)\n",
    "\n",
    "        output = F.log_softmax(out, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv11): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv12): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv13): Conv2d(1, 16, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv14): Conv2d(1, 16, kernel_size=(9, 9), stride=(2, 2))\n",
       "  (conv21): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv22): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv23): Conv2d(16, 32, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (conv24): Conv2d(16, 32, kernel_size=(9, 9), stride=(2, 2))\n",
       "  (conv31): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv32): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv33): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (conv34): Conv2d(32, 64, kernel_size=(9, 9), stride=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc11): Linear(in_features=7744, out_features=256, bias=True)\n",
       "  (fc12): Linear(in_features=5184, out_features=256, bias=True)\n",
       "  (fc13): Linear(in_features=3136, out_features=256, bias=True)\n",
       "  (fc14): Linear(in_features=1600, out_features=256, bias=True)\n",
       "  (fc21): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc22): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc23): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc24): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc33): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_1 = []\n",
    "losses_2 = []\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send the image, target to the device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # flush out the gradients stored in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # pass the image to the model and assign the output to variable named output\n",
    "        output = model(data)\n",
    "        # calculate the loss (use nll_loss in pytorch)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # do a backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            losses_1.append(loss.item())\n",
    "            losses_2.append(100. * batch_idx / len(train_loader))\n",
    "\n",
    "accuracy = []\n",
    "avg_loss = []\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "          \n",
    "            # send the image, target to the device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # pass the image to the model and assign the output to variable named output\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "          \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    avg_loss.append(test_loss)\n",
    "    accuracy.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Net().to(device)\n",
    "learning_rate = []\n",
    "def adjust_learning_rate(optimizer, iter, each):\n",
    "    # sets the learning rate to the initial LR decayed by 0.1 every 'each' iterations\n",
    "    lr = 0.001 * (0.95 ** (iter // each))\n",
    "    state_dict = optimizer.state_dict()\n",
    "    for param_group in state_dict['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.load_state_dict(state_dict)\n",
    "    print(\"Learning rate = \",lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(0,120):\n",
    "  lr = adjust_learning_rate(optimizer, epoch, 1.616)\n",
    "  learning_rate.append(lr)\n",
    "  train(model, device, train_dataloader, optimizer, epoch)\n",
    "  test(model, device, test_dataloader)\n",
    "  if (epoch + 1) % 10 == 0:\n",
    "    torch.save(\n",
    "        {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, \n",
    "        f\"mnist_model_padding_ckpt_E{epoch}.pth\"\n",
    "    )\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, \n",
    "    \"mnist_model_ckpt.pth\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soccernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
